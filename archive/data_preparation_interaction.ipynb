{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jCTtxTZ2SBGN"
   },
   "outputs": [],
   "source": [
    "# filtered or not\n",
    "# word2vec choice\n",
    "# with images or not.\n",
    "# pca\n",
    "\n",
    "\n",
    "# main function is f(user, business)\n",
    "# user2avgVec # inclusive of current business\n",
    "# user2avgHistVec # exclusive of current business\n",
    "# business2avgVec # inclusive of current user\n",
    "# business2avgHistVec # exclusive of current user\n",
    "\n",
    "# with or without t filtered json?\n",
    "# with or without currents?\n",
    "# with or without pics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uqhtH5L27Ufq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBpZE7xbVj4H",
    "outputId": "da06c17a-ae56-4651-9e1d-7095d4ca76f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup  \n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#import geopandas as gpd|\n",
    "#import shapely\n",
    "import gensim\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "#mounted_path_folder = '/content/drive'\n",
    "#drive.mount(mounted_path_folder, force_remount=True)\n",
    "\n",
    "import sys\n",
    "path_folder = \"/content/drive/MyDrive/dsprojects/dsproject_grev/\" # parent of current src folder\n",
    "path_folder = (os.path.abspath(os.path.join((os.path.abspath(os.path.join(os.getcwd(), os.pardir))),os.pardir)))\n",
    "sys.path.insert(0, path_folder+\"/src/features/\")\n",
    "import util\n",
    "\n",
    "from word2vec_recipe import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dPbQuclh6CoL"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "#vggnet_model = VGG16()\n",
    "url_header_pics = \"https://lh5.googleusercontent.com/p/\"\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "path_file_name = os.path.join(path_folder, \"data\", \"temp\",\"business_user2picVec.pkl\")\n",
    "\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z99RCySZpcOk"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RpsUrxU9zRUd"
   },
   "outputs": [],
   "source": [
    "path_folder = \"/content/drive/MyDrive/dsprojects/dsproject_grev/\" # parent of current src folder\n",
    "path_folder = (os.path.abspath(os.path.join((os.path.abspath(os.path.join(os.getcwd(), os.pardir))),os.pardir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2e9fEv-bbJU"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vocab_column_names = [\"ingredients\"]+[\"name\"]\n",
    "file_name_word2vec = \"word2vec_\"+(\"_\".join(vocab_column_names))+\".pkl\"\n",
    "path_file_word2vec = os.path.join(path_folder, \"data\",\"temp\",file_name_word2vec)\n",
    "with open(path_file_word2vec, \"rb\") as f:\n",
    "    recipe_word2vec = pickle.load(f)\n",
    "\n",
    "vocab_column_names = [\"ingredients\"]#+[\"name\"]\n",
    "file_name_tfidf = \"tfidf_\"+(\"_\".join(vocab_column_names))+\".pkl\"\n",
    "path_file_tfidf = os.path.join(path_folder, \"data\",\"temp\",file_name_tfidf)\n",
    "with open(path_file_tfidf, \"rb\") as f:\n",
    "    recipe_tfidf = pickle.load(f)\n",
    "\n",
    "DATASET_VERSION = [\"FULL\", \"FILTER\"][0]  \n",
    "DROP_ONE_TIME_APPEARANCES = True\n",
    "DROP_SEPARATED_ONE_TIME_APPEARANCES = False\n",
    "USING_PICS = False\n",
    "\n",
    "if DATASET_VERSION == \"FILTER\":\n",
    "    path_folder_data = os.path.join(path_folder,\"data\",\"raw\",\"filter_all_t.json\")\n",
    "else:\n",
    "    path_folder_data = os.path.join(path_folder,\"data\",\"raw\",\"image_review_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhoNkz6WoTSz",
    "outputId": "89cb949d-94dc-41bb-d2f5-46dceda0ee72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487747it [03:47, 6552.90it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(path_folder_data) as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        data.append(eval(line))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HwfOa9KNrKmM"
   },
   "outputs": [],
   "source": [
    "# Filter or Full\n",
    "# Pics or no Pics\n",
    "# Drop one-time-appearing business_ids/user_ids: (seperately, or conjoined?) (for example, the business_id is 1-time, but the user_id has already appeared many times before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-n8LViKYtFv",
    "outputId": "361ad05f-62e1-4cba-8677-255be13f2265"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1487747/1487747 [00:10<00:00, 147777.41it/s]\n"
     ]
    }
   ],
   "source": [
    "if DATASET_VERSION == \"FILTER\":\n",
    "    train_reviews = (data[0][\"train\"])\n",
    "    test_reviews = (data[0][\"test\"])\n",
    "else:\n",
    "    train_reviews = data\n",
    "# 3 tables:\n",
    "# 1 for business_id, user_id, review_text, rating\n",
    "# 1 for pics: business_id, user_id, pic\n",
    "# 1 for external history review: business_id, user_id, review_text\n",
    "\n",
    "#reviews_df = pd.DataFrame(data=None, columns=[\"business_id\",\"user_id\",\"rating\",\"review_text\"])\n",
    "#history_reviews_df = pd.DataFrame(data=None, columns=[\"business_id\",\"user_id\",\"review_text\"])\n",
    "#pics_df = pd.DataFrame(data=None, columns=[\"business_id\",\"user_id\",\"pics\"])\n",
    "\n",
    "reviews_df_business_id_column_list = []\n",
    "reviews_df_user_id_column_list = []\n",
    "reviews_df_rating_column_list = []\n",
    "reviews_df_review_text_column_list = []\n",
    "\n",
    "history_reviews_df_business_id_column_list = []\n",
    "history_reviews_df_user_id_column_list = []\n",
    "history_reviews_df_review_text_column_list = []\n",
    "\n",
    "pics_df_business_id_column_list = []\n",
    "pics_df_user_id_column_list = []\n",
    "pics_df_pics_column_list = []\n",
    "\n",
    "\n",
    "for review in tqdm.tqdm(train_reviews):\n",
    "    temp_business_id = None\n",
    "    temp_user_id = None\n",
    "    temp_rating = None\n",
    "    temp_review_text = None\n",
    "    temp_pics = None\n",
    "    temp_history_reviews = None\n",
    "\n",
    "    if \"business_id\" in review:\n",
    "        temp_business_id = review[\"business_id\"]\n",
    "    if \"user_id\" in review:\n",
    "        temp_user_id = review[\"user_id\"]\n",
    "    if \"rating\" in review:\n",
    "        temp_rating = review[\"rating\"]\n",
    "    if \"review_text\" in review:\n",
    "        temp_review_text = review[\"review_text\"]\n",
    "    if \"pics\" in review:\n",
    "        temp_pics = review[\"pics\"]\n",
    "        if DATASET_VERSION == \"FULL\":\n",
    "             temp_pics = [x[\"id\"] for x in (temp_pics)]\n",
    "    if \"history_reviews\" in review:\n",
    "        temp_history_reviews = review[\"history_reviews\"]\n",
    "    \n",
    "    reviews_df_business_id_column_list.append(temp_business_id)\n",
    "    reviews_df_user_id_column_list.append(temp_user_id)\n",
    "    reviews_df_rating_column_list.append(temp_rating)\n",
    "    reviews_df_review_text_column_list.append(temp_review_text)\n",
    "\n",
    "    if USING_PICS == True:\n",
    "        if (temp_pics is None)==False:\n",
    "            temp_pics_len = len(temp_pics)\n",
    "            pics_df_business_id_column_list.extend([temp_business_id for _ in range(temp_pics_len)])\n",
    "            pics_df_user_id_column_list.extend([temp_user_id for _ in range(temp_pics_len)])\n",
    "            pics_df_pics_column_list.extend(temp_pics)\n",
    "\n",
    "    #if (temp_history_reviews is None)==False:\n",
    "    #    temp_history_reviews_len = len(temp_history_reviews)\n",
    "    #    history_reviews_df_business_id_column_list.extend([x[0].split(\"_\")[1] for x in temp_history_reviews])\n",
    "    #    history_reviews_df_user_id_column_list.extend([temp_user_id for _ in range(temp_history_reviews_len)])\n",
    "    #    history_reviews_df_review_text_column_list.extend([x[1] for x in temp_history_reviews])\n",
    "\n",
    "\n",
    "reviews_df = pd.DataFrame()\n",
    "reviews_df[\"business_id\"] = reviews_df_business_id_column_list\n",
    "reviews_df[\"user_id\"] = reviews_df_user_id_column_list\n",
    "reviews_df[\"rating\"] = reviews_df_rating_column_list\n",
    "reviews_df[\"review_text\"] = reviews_df_review_text_column_list\n",
    "\n",
    "#history_reviews_df = pd.DataFrame()\n",
    "#history_reviews_df[\"business_id\"] = history_reviews_df_business_id_column_list\n",
    "#history_reviews_df[\"user_id\"] = history_reviews_df_user_id_column_list\n",
    "#history_reviews_df[\"review_text\"] = history_reviews_df_review_text_column_list\n",
    "if USING_PICS == True:\n",
    "    pics_df = pd.DataFrame()\n",
    "    pics_df[\"business_id\"] = pics_df_business_id_column_list\n",
    "    pics_df[\"user_id\"] = pics_df_user_id_column_list\n",
    "    pics_df[\"pics\"] = pics_df_pics_column_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CY6DUPUIy6ZG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "business_id_label_encoder = LabelEncoder()\n",
    "user_id_label_encoder = LabelEncoder()\n",
    "\n",
    "business_id_label_encoder.fit(reviews_df[\"business_id\"])\n",
    "user_id_label_encoder.fit(reviews_df[\"user_id\"])\n",
    "\n",
    "reviews_df[\"business_id\"] = business_id_label_encoder.transform(reviews_df[\"business_id\"])\n",
    "reviews_df[\"user_id\"] = user_id_label_encoder.transform(reviews_df[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xR3jrfzdZnEH"
   },
   "outputs": [],
   "source": [
    "del data\n",
    "del reviews_df_business_id_column_list\n",
    "del reviews_df_user_id_column_list\n",
    "del reviews_df_rating_column_list\n",
    "del reviews_df_review_text_column_list\n",
    "del train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend the filt was stacked with train test and pics words added already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(text_column):\n",
    "    non_alphanumeric = string.punctuation # constant\n",
    "    text_column = text_column.str.lower()\n",
    "    text_column = text_column.str.translate(str.maketrans(non_alphanumeric, \" \"*len(non_alphanumeric)))\n",
    "    return text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = reviews_df\n",
    "temp_df[\"review_id\"] = np.arange(temp_df.shape[0])\n",
    "id_df = temp_df[[\"user_id\",\"business_id\",\"review_id\"]]\n",
    "review_df = temp_df[[\"review_id\",\"review_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df[\"review_text\"] = clean_text_column(review_df[\"review_text\"]).str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTextWords_df = review_df.explode(\"review_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43678604"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviewTextWords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTextWords_df = reviewTextWords_df[~(reviewTextWords_df[\"review_text\"].isin(stop_words))]\n",
    "reviewTextWords_df = reviewTextWords_df[(reviewTextWords_df[\"review_text\"].isin(recipe_word2vec.wv.vocab))]\n",
    "reviewTextWords_df = reviewTextWords_df[(reviewTextWords_df[\"review_text\"].isin(recipe_tfidf.vocab2idx))]\n",
    "reviewTextWords_df = reviewTextWords_df[(reviewTextWords_df[\"review_text\"] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[(review_df[\"review_id\"].isin(pd.unique(reviewTextWords_df[\"review_id\"])))]\n",
    "# Remove double singletons\n",
    "\n",
    "\n",
    "business_id_frequency = id_df.groupby([\"business_id\"],as_index=False)[\"business_id\"].count()\n",
    "user_id_frequency = id_df.groupby([\"user_id\"],as_index=False)[\"user_id\"].count()\n",
    "one_time_business_id_list = business_id_frequency[business_id_frequency[\"count\"] == 1][\"business_id\"]\n",
    "one_time_user_id_list = user_id_frequency[user_id_frequency[\"count\"] == 1][\"user_id\"]\n",
    "\n",
    "if DROP_ONE_TIME_APPEARANCES == True:\n",
    "    id_df = id_df[\n",
    "        ~( (id_df[\"business_id\"].isin(one_time_business_id_list)) & (id_df[\"user_id\"].isin(one_time_user_id_list)))\n",
    "    ]\n",
    "del business_id_frequency\n",
    "del user_id_frequency\n",
    "del one_time_business_id_list\n",
    "del one_time_user_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for word in recipe_word2vec.wv.vocab\n",
    "         if ((word in recipe_tfidf.vocab2idx) and (word not in stop_words))\n",
    "        ]\n",
    "vecs = [recipe_word2vec.wv[word] for word in vocab]\n",
    "word2vec_df = pd.DataFrame()\n",
    "word2vec_df[\"review_text\"] = vocab\n",
    "word2vec_df[\"vec\"] = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTextWords_df[\"word_count\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTextWord2vec_df = reviewTextWords_df.merge(word2vec_df, on=[\"review_text\"])\n",
    "\n",
    "reviewTextWord2vec_df.sort_values([\"review_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTextWord2vec_df_agg = (\n",
    "    reviewTextWord2vec_df.groupby([\"review_id\"], as_index=False).agg(\n",
    "        {\"vec\": list, \"word_count\": np.sum})\n",
    ")\n",
    "reviewTextWord2vec_df_agg[\"vec\"] = reviewTextWord2vec_df_agg[\"vec\"].apply(lambda x: np.sum(np.array(x), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_review_df = id_df.merge(reviewTextWord2vec_df_agg, on=[\"review_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.max(id_review_df[\"vec\"].apply(lambda x: np.max(np.abs(x)))/id_review_df[\"word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_review_df_agg_user = (\n",
    "    id_review_df.groupby([\"user_id\"], as_index=False).agg(\n",
    "        {\"vec\": list, \"word_count\": np.sum})\n",
    ")\n",
    "id_review_df_agg_user[\"vec\"] = id_review_df_agg_user[\"vec\"].apply(lambda x: np.sum(np.array(x), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assert id_review_df_agg_user.shape[0] == 29585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_review_df_agg_business = (\n",
    "    id_review_df.groupby([\"business_id\"], as_index=False).agg(\n",
    "        {\"vec\": list, \"word_count\": np.sum})\n",
    ")\n",
    "id_review_df_agg_business[\"vec\"] = id_review_df_agg_business[\"vec\"].apply(lambda x: np.sum(np.array(x), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assert id_review_df_agg_business.shape[0] == 27734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id2vec_df = id_review_df.copy()\n",
    "id2vec_df = id2vec_df.rename(columns={\"vec\": \"vec_curr\", \"word_count\": \"word_count_curr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.mean(id2vec_df[\"word_count_curr\"]>0) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_df = id2vec_df.merge(id_review_df_agg_user, on=[\"user_id\"])\n",
    "id2vec_df[\"vec\"] -= id2vec_df[\"vec_curr\"]\n",
    "id2vec_df[\"word_count\"] -=id2vec_df[\"word_count_curr\"]\n",
    "id2vec_df = id2vec_df.rename(columns={\"vec\": \"user_vec\", \"word_count\": \"user_word_count\"})\n",
    "id2vec_df[\"user_vec\"] /= id2vec_df[\"user_word_count\"]\n",
    "id2vec_df = id2vec_df[id2vec_df[\"user_word_count\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_df = id2vec_df.merge(id_review_df_agg_business, on=[\"business_id\"])\n",
    "id2vec_df[\"vec\"] -= id2vec_df[\"vec_curr\"]\n",
    "id2vec_df[\"word_count\"] -= id2vec_df[\"word_count_curr\"]\n",
    "id2vec_df = id2vec_df.rename(columns={\"vec\": \"business_vec\", \"word_count\": \"business_word_count\"})\n",
    "id2vec_df[\"business_vec\"] /= id2vec_df[\"business_word_count\"]\n",
    "id2vec_df = id2vec_df[id2vec_df[\"business_word_count\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#np.max(id2vec_df[\"user_vec\"].apply(lambda x: np.max(np.abs(x)))/id2vec_df[\"user_word_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#np.max(id2vec_df[\"business_vec\"].apply(lambda x: np.max(np.abs(x)))/id2vec_df[\"business_word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2vec_df = id2vec_df.drop(columns=[\"vec_curr\",\"word_count_curr\",\"user_word_count\",\"business_word_count\"])\n",
    "concat_vecs = (\n",
    "    id2vec_df[\"user_vec\"].apply(list) + id2vec_df[\"business_vec\"].apply(list)\n",
    ")\n",
    "train_X_curr_excluded_from_average = np.array(concat_vecs.values.tolist())\n",
    "train_y_curr_excluded_from_average = np.ones(train_X_curr_excluded_from_average.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#np.max(train_X_curr_excluded_from_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(id_review_df_agg_user[\"vec\"].apply(lambda x: np.max(np.abs(x)))/id_review_df_agg_user[\"word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(id_review_df_agg_business[\"vec\"].apply(lambda x: np.max(np.abs(x)))/id_review_df_agg_business[\"word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_review_df_agg_user[\"vec\"] /= id_review_df_agg_user[\"word_count\"]\n",
    "id_review_df_agg_business[\"vec\"] /= id_review_df_agg_business[\"word_count\"]\n",
    "\n",
    "user_mapper = id_review_df_agg_user[[\"user_id\",\"vec\"]].set_index(\"user_id\")[\"vec\"].to_dict()\n",
    "business_mapper = id_review_df_agg_business[[\"business_id\", \"vec\"]].set_index(\"business_id\")[\"vec\"].to_dict()\n",
    "\n",
    "valid_user_id_column = id2vec_df[\"user_id\"].values.tolist()\n",
    "valid_business_id_column = id2vec_df[\"business_id\"].values.tolist()\n",
    "#valid_user_id_column = id_review_df[\"user_id\"].values.tolist()\n",
    "#valid_business_id_column = id_review_df[\"business_id\"].values.tolist()\n",
    "\n",
    "pair_existence_checker = defaultdict(lambda: -1)\n",
    "for pair in list(zip(valid_user_id_column,valid_business_id_column)):\n",
    "    pair_existence_checker[pair] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.unique(valid_user_id_column)),len(pd.unique(valid_business_id_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.unique(valid_user_id_column)),len(pd.unique(valid_business_id_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pd.unique(valid_user_id_column)), should be 28835\n",
    "# len(pd.unique(valid_business_id_column)), should be 15285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unseen sampling (PCA verifies that this is a valid operation)\n",
    "non_existing_interaction_pairs = []\n",
    "pair_non_existence_checker = defaultdict(lambda: -1)\n",
    "\n",
    "sampled_user_id_list = np.random.choice(valid_user_id_column, size=len(set(valid_user_id_column)))\n",
    "business_id_list_ = list(set(valid_business_id_column)).copy()\n",
    "\n",
    "business_amount_per_user = train_X_curr_excluded_from_average.shape[0] // (len(set(valid_user_id_column)))\n",
    "\n",
    "max_sampling_amounts = [\n",
    "    #np.floor(business_amount_per_user), \n",
    "    np.ceil(business_amount_per_user)\n",
    "    ]\n",
    "train_X_averages_with_curr_bc_unseen = []\n",
    "train_y_averages_with_curr_bc_unseen = []\n",
    "concat_embeddings_maxes = []\n",
    "for temp_user_id in tqdm.tqdm(sampled_user_id_list):\n",
    "    sampled_amount = 0\n",
    "    business_id_iterator = 0\n",
    "    np.random.shuffle(business_id_list_)\n",
    "    max_sampling_amount = np.random.choice(max_sampling_amounts)\n",
    "    while sampled_amount < max_sampling_amount:\n",
    "        temp_business_id = business_id_list_[business_id_iterator]\n",
    "        pair = [temp_user_id, temp_business_id]\n",
    "        if ((pair_existence_checker[tuple(pair)] == -1) and (pair_non_existence_checker[tuple(pair)] == -1)):\n",
    "            pair_non_existence_checker[tuple(pair)] = 1\n",
    "            #non_existing_interaction_pairs.append(pair)\n",
    "            concat_embeddings = list(user_mapper[temp_user_id]) + list(business_mapper[temp_business_id])\n",
    "            train_X_averages_with_curr_bc_unseen.append(concat_embeddings)\n",
    "            train_y_averages_with_curr_bc_unseen.append(0)\n",
    "            sampled_amount += 1\n",
    "        business_id_iterator += 1\n",
    "assert all([(pair_existence_checker[tuple(pair)]==-1) for pair in non_existing_interaction_pairs])\n",
    "\n",
    "train_X_averages_with_curr_bc_unseen = np.array(train_X_averages_with_curr_bc_unseen)\n",
    "train_y_averages_with_curr_bc_unseen = np.array(train_y_averages_with_curr_bc_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_interaction_pairs = train_X_curr_excluded_from_average\n",
    "non_existing_interaction_pairs = train_X_averages_with_curr_bc_unseen\n",
    "train_X_seen = train_X_curr_excluded_from_average\n",
    "train_X_unseen = train_X_averages_with_curr_bc_unseen\n",
    "train_y_seen = train_y_curr_excluded_from_average\n",
    "train_y_unseen = train_y_averages_with_curr_bc_unseen\n",
    "\n",
    "train_dataset_size = len(existing_interaction_pairs)+len(non_existing_interaction_pairs)\n",
    "train_dataset_indexer = np.arange(train_dataset_size)\n",
    "np.random.shuffle(train_dataset_indexer)\n",
    "\n",
    "train_X = np.vstack((train_X_curr_excluded_from_average,train_X_averages_with_curr_bc_unseen))\n",
    "train_y = np.hstack((train_y_curr_excluded_from_average,train_y_averages_with_curr_bc_unseen))\n",
    "\n",
    "train_X = train_X[train_dataset_indexer]\n",
    "train_y = train_y[train_dataset_indexer]\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "temp_X_pca_input = train_X\n",
    "temp_y_pca_input = train_y\n",
    "\n",
    "Z = pca.fit_transform(temp_X_pca_input)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(Z[:,0], Z[:,1],c=temp_y_pca_input, alpha=0.1, s=5, cmap=\"winter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "hP-IJWvUYP5t",
    "outputId": "fa3c4a94-880b-44ab-9c60-6f26d1734fb4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=500, class_weight=\"balanced\", C=0.5)\n",
    "log_reg.fit(X_train, y_train)\n",
    "lr_y_pred = log_reg.predict(X_val)\n",
    "\n",
    "y_true = y_val\n",
    "y_pred = lr_y_pred\n",
    "conf_matrix = confusion_matrix(y_true, y_pred,normalize=\"true\")\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "ax = sns.heatmap(conf_matrix, annot=True) #notation: \"annot\" not \"annote\"\n",
    "y_lims = ax.get_ylim()\n",
    "ax.set_ylim(sum(y_lims), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_y_pred = dt.predict(X_val)\n",
    "\n",
    "y_true = y_val\n",
    "y_pred = lr_y_pred\n",
    "conf_matrix = confusion_matrix(y_true, y_pred,normalize=\"true\")\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "ax = sns.heatmap(conf_matrix, annot=True) #notation: \"annot\" not \"annote\"\n",
    "y_lims = ax.get_ylim()\n",
    "ax.set_ylim(sum(y_lims), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "aE_6UAccGOWw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11795/11795 [==============================] - 30s 3ms/step - loss: 0.6041 - accuracy: 0.6731\n",
      "Epoch 2/5\n",
      "11795/11795 [==============================] - 29s 2ms/step - loss: 0.5812 - accuracy: 0.6955\n",
      "Epoch 3/5\n",
      "11795/11795 [==============================] - 29s 2ms/step - loss: 0.5739 - accuracy: 0.7018\n",
      "Epoch 4/5\n",
      "11795/11795 [==============================] - 29s 2ms/step - loss: 0.5697 - accuracy: 0.7049\n",
      "Epoch 5/5\n",
      "11795/11795 [==============================] - 29s 2ms/step - loss: 0.5665 - accuracy: 0.7075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ae564c69c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(train_X.shape[1],), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-5WdYIn5JW4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12286/12286 [==============================] - 19s 2ms/step - loss: 0.5677 - accuracy: 0.7069\n",
      "12286/12286 [==============================] - 16s 1ms/step\n",
      "Val Loss:  0.5676828026771545 , Test Accuracy:  0.7068740725517273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.0, 0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "#assert np.all(predictions >= 0)\n",
    "#assert np.all(predictions <= 1)\n",
    "#accuracy = np.mean(y_val==np.squeeze((predictions >= 0.5).astype(int)))\n",
    "\n",
    "print(\"Val Loss: \", val_loss, \", Test Accuracy: \", val_acc)\n",
    "y_true = y_val\n",
    "y_pred = np.squeeze((predictions >= 0.5).astype(int))\n",
    "conf_matrix = confusion_matrix(y_true, y_pred,normalize=\"true\")\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "ax = sns.heatmap(conf_matrix, annot=True) #notation: \"annot\" not \"annote\"\n",
    "y_lims = ax.get_ylim()\n",
    "ax.set_ylim(sum(y_lims), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PE7mW34yTVkD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkCAUPb-TVM7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSsvhK-7TUtg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkVSOvV8TUqv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01LKTC69TUn1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKMz6QB7TUhl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfZ2LSY8TUUX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22j2XX7K_k_"
   },
   "source": [
    "# VISUALIZATION ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63MPQKdD1Ozf"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "def plot_silhoutte_scores(embeddings):\n",
    "    range_n_clusters = [20, 50, 100, 250, 500, 1000]\n",
    "    silhouette_avg = []\n",
    "    for num_clusters in tqdm.tqdm(range_n_clusters):   \n",
    "        # initialise kmeans\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(embeddings)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # silhouette score\n",
    "        score = silhouette_score(embeddings, cluster_labels)\n",
    "        silhouette_avg.append(score)\n",
    "        print(score)\n",
    "    plt.plot(range_n_clusters,silhouette_avg,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Silhouette score') \n",
    "    plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()\n",
    "# plot_silhoutte_scores(user_embeddings)\n",
    "# silhoutte scores reveal that user_embeddings is poor to k-means cluster on (highly overlapped): increasing K worsens the silhoutte score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlLBJ7467iQy"
   },
   "outputs": [],
   "source": [
    "your_word_vector = business_embeddings[0]\n",
    "recipe_word2vec.most_similar(positive=[your_word_vector], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bGHzyXH_jZl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxz0_IOQ5iXf"
   },
   "outputs": [],
   "source": [
    "def clean_text_column(text_column):\n",
    "    non_alphanumeric = string.punctuation # constant\n",
    "    text_column = text_column.str.lower()\n",
    "    text_column = text_column.str.translate(str.maketrans(non_alphanumeric, \" \"*len(non_alphanumeric)))\n",
    "    return text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo9HWo9j3yoC"
   },
   "outputs": [],
   "source": [
    "i = 17\n",
    "# 10\n",
    "print(\"Business id:\", i)\n",
    "print(recipe_word2vec.most_similar(positive=[business_embeddings[i]], topn=1)[0][0])\n",
    "def get_words_of_business_id(business_id):\n",
    "    business_id_df = reviews_df[reviews_df[\"business_id\"]==business_id]\n",
    "    review_text_column = business_id_df[\"review_text\"]\n",
    "    review_text_column = clean_text_column(review_text_column)\n",
    "    review_text_column = review_text_column.apply(\n",
    "        lambda x: \" \".join([\n",
    "            word for word in x.split(\" \") \n",
    "            if word in recipe_tfidf.vocab2idx\n",
    "            ])+\" \")\n",
    "    review_text = review_text_column.values.sum()\n",
    "    review_text_set = pd.Series(review_text.split(\" \")).value_counts()\n",
    "    review_text_set\n",
    "    print(review_text_set)\n",
    "\n",
    "get_words_of_business_id(business_id_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_zfD7sW7K_G"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"The representative word for Restaurant\", business_id_list[i], \"is :\\n\", \n",
    "      recipe_word2vec.most_similar(positive=[business_embeddings[i]], topn=1)[0][0])\n",
    "print(\"\\nThe reviews for this restaurant are as follows:\\n\")\n",
    "(reviews_df[reviews_df[\"business_id\"]==business_id_list[i]][\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqlTSWjEbPwT"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "eps = 3\n",
    "min_samples = 1\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan.fit(business_embeddings)\n",
    "business_labels = dbscan.labels_\n",
    "print(len((business_labels)))\n",
    "assert len(business_id_list) == len(business_labels)\n",
    "print(len(set(business_labels)))\n",
    "\n",
    "eps = 3\n",
    "min_samples = 1\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan.fit(user_embeddings)\n",
    "user_labels = dbscan.labels_\n",
    "print(len((user_labels)))\n",
    "assert len(user_id_list) == len(user_labels)\n",
    "\n",
    "print(len(set(user_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uIrOF2TK-hG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgwhsdPdK-bo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpbydb9ZPKwQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3Y52k_RhVc3"
   },
   "outputs": [],
   "source": [
    "business_embeddings_ = StandardScaler().fit_transform(business_embeddings)\n",
    "user_embeddings_ = StandardScaler().fit_transform(user_embeddings)\n",
    "pca = PCA(50)\n",
    "tsne = TSNE(n_components=2, perplexity=100, n_iter=250)\n",
    "Z_business = business_embeddings_\n",
    "Z_business = pca.fit_transform(Z_business)\n",
    "Z_business = tsne.fit_transform(Z_business)\n",
    "#Z_business = umap0.fit_transform(Z_business)\n",
    "\n",
    "pca = PCA(50)\n",
    "tsne = TSNE(n_components=2, perplexity=100, n_iter=250)\n",
    "Z_user = user_embeddings_\n",
    "Z_user = pca.fit_transform(Z_user)\n",
    "Z_user = tsne.fit_transform(Z_user)\n",
    "#Z_user = umap0.fit_transform(Z_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaIbCZfx5u2y"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "axs[0].scatter(Z_business[:,0], Z_business[:,1], alpha=0.5,s=1, c=business_labels,cmap=\"Spectral\")\n",
    "axs[0].set_title(\"Business Embeddings\")\n",
    "#from matplotlib import cm\n",
    "#colormap = cm.hsv(range(20))\n",
    "\n",
    "\n",
    "axs[1].scatter(Z_user[:,0], Z_user[:,1], alpha=0.5,s=1, c=user_labels,cmap=\"Spectral\")#, label=word_labels)\n",
    "axs[1].set_title(\"User Embeddings\")\n",
    "#axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRXoxPS9uoMl"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(10)\n",
    "kmeans.fit(business_embeddings)\n",
    "business_labels = kmeans.labels_\n",
    "print(len((business_labels)))\n",
    "print(len(set(business_labels)))\n",
    "\n",
    "kmeans = KMeans(20)\n",
    "kmeans.fit(user_embeddings)\n",
    "user_labels = kmeans.labels_\n",
    "print(len((user_labels)))\n",
    "print(len(set(user_labels)))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "axs[0].scatter(Z_business[:,0], Z_business[:,1], alpha=0.5,s=1, c=business_labels,cmap=\"tab10\")\n",
    "axs[0].set_title(\"Business Embeddings\")\n",
    "#from matplotlib import cm\n",
    "#colormap = cm.hsv(range(20))\n",
    "\n",
    "\n",
    "axs[1].scatter(Z_user[:,0], Z_user[:,1], alpha=0.5,s=1, c=user_labels,cmap=\"tab20\")#, label=word_labels)\n",
    "axs[1].set_title(\"User Embeddings\")\n",
    "#axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEn7I11Yx7y2"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "axs[0].scatter(Z_business[:,0], Z_business[:,1], alpha=0.5,s=1, c='r',cmap=\"tab10\")\n",
    "axs[0].set_title(\"Business Embeddings\")\n",
    "#from matplotlib import cm\n",
    "#colormap = cm.hsv(range(20))\n",
    "\n",
    "\n",
    "axs[1].scatter(Z_user[:,0], Z_user[:,1], alpha=0.5,s=1, c='r',cmap=\"tab20\")#, label=word_labels)\n",
    "axs[1].set_title(\"User Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWII7Ea9KWS_"
   },
   "outputs": [],
   "source": [
    "def get_label_data(labels, id_list, id2vec, id_column_name, reviews_df):\n",
    "    labels_set = set(labels)\n",
    "    labels_representative_words = []\n",
    "    labels_representative_embeddings = []\n",
    "    labels_vocab_frequency = []\n",
    "    for label in labels_set:\n",
    "        temp_idxs = np.where(np.array(labels)==label)[0]\n",
    "        embeddings_group = []\n",
    "        group_vocab = \" \"\n",
    "        for temp_idx in temp_idxs:\n",
    "            temp_id = id_list[temp_idx]\n",
    "            #for key in id2vec[temp_id]: embeddings_group += id2vec[(id_list[temp_idx])][key];\n",
    "            ##\n",
    "            temp_id_df = reviews_df[reviews_df[id_column_name]==temp_id]\n",
    "            review_text_column = temp_id_df[\"review_text\"]\n",
    "            review_text_column = clean_text_column(review_text_column)\n",
    "            review_text_column = review_text_column.apply(\n",
    "                lambda x: \" \".join([\n",
    "                    word for word in x.split(\" \") \n",
    "                    if word in recipe_tfidf.vocab2idx\n",
    "                    ])+\" \")\n",
    "            \n",
    "            combined_review_texts_of_id = review_text_column.values.sum() # get all re\n",
    "            group_vocab += (combined_review_texts_of_id+\" \")\n",
    "            ##\n",
    "        group_vocab_list = group_vocab.split(\" \")\n",
    "        group_vocab_list = [word for word in group_vocab_list if word != \"\"]\n",
    "        group_vocab_frequency = pd.Series(group_vocab_list).value_counts()\n",
    "        labels_vocab_frequency.append(group_vocab_frequency)\n",
    "\n",
    "\n",
    "        #embeddings_group = np.array(embeddings_group)\n",
    "        #labels_representative_embedding = embeddings_group.mean(axis=0)\n",
    "        #labels_representative_embeddings.append(labels_representative_embedding)\n",
    "\n",
    "        #labels_representative_word = recipe_word2vec.most_similar(positive=[labels_representative_embedding], topn=1)\n",
    "        #labels_representative_words.append(labels_representative_word[0][0])\n",
    "\n",
    "\n",
    "    return labels_set, labels_representative_words, labels_representative_embeddings, labels_vocab_frequency\n",
    "#output_labels_data = get_label_data(business_labels, business_id_list, business_user2reviewTextVec, \"business_id\", reviews_df)\n",
    "#labels_set, labels_representative_words, labels_representative_embeddings, labels_vocab_frequency = output_labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w91T9-fC-A8-"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-JM22n0DESM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjtDyc86FB4f"
   },
   "outputs": [],
   "source": [
    "learned_mapping = dict(zip(list(recipe_tfidf.vocab2idx.keys()),list(range(len(recipe_tfidf.vocab2idx)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "618fZuahCsyB"
   },
   "outputs": [],
   "source": [
    "label_column_name = \"business_label\"\n",
    "business_labeler = pd.DataFrame()\n",
    "business_labeler[label_column_name] = business_id_list\n",
    "business_labeler[label_column_name] = business_labels\n",
    "temp_reviews_df = reviews_df.merge(business_labeler, on=[label_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Hzw-o3Fl01m"
   },
   "outputs": [],
   "source": [
    "label_column_name = \"user_label\"\n",
    "user_labeler = pd.DataFrame()\n",
    "user_labeler[\"user_id\"] = user_id_list\n",
    "user_labeler[label_column_name] = user_labels\n",
    "temp_reviews_df = reviews_df.merge(user_labeler, on=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKLSXPtERtes"
   },
   "outputs": [],
   "source": [
    "### Move to Vectorizer_Builder (collapser version 1)\n",
    "learned_mapping = dict(zip(list(recipe_tfidf.vocab2idx.keys()),list(range(len(recipe_tfidf.vocab2idx)))))\n",
    "\n",
    "temp_labels = list(pd.unique(temp_reviews_df[label_column_name]))\n",
    "\n",
    "review_text_column_ = []\n",
    "\n",
    "for label in temp_labels:\n",
    "    label_all_text = \"\"\n",
    "    temp_reviews_df_label = temp_reviews_df[temp_reviews_df[label_column_name]==label]\n",
    "    temp_review_text_column = temp_reviews_df_label[\"review_text\"]\n",
    "    temp_review_text_column = clean_text_column(temp_review_text_column)\n",
    "    for row in tqdm.tqdm(list(temp_review_text_column.values)):\n",
    "        label_all_text += (\" \"+row)\n",
    "    review_text_column_.append(label_all_text)\n",
    "learned_tv = Pipeline([('count', CountVectorizer(vocabulary=learned_mapping)),\n",
    "                       ('tfidf', TfidfTransformer())])\n",
    "learned_tv.fit(review_text_column_)\n",
    "tv_output_collapsed = learned_tv.transform(review_text_column_)\n",
    "#tv_output_collapsed = tv_output_collapsed.tocsc().A\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_top_n_from_vector(feature_names, sorted_items, top_n=10):\n",
    "    sorted_items = sorted_items[:top_n]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score,3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiEWdloTmXRv"
   },
   "outputs": [],
   "source": [
    "# Now doing USERS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIJ6wN_tlHNQ"
   },
   "outputs": [],
   "source": [
    "feature_names = learned_tv.named_steps[\"count\"].get_feature_names()\n",
    "i = 19\n",
    "print(\"Group\", temp_labels[i], \"Keywords:\" )\n",
    "output_label_temp = learned_tv.transform([review_text_column_[i]])\n",
    "sorted_items = sort_coo(output_label_temp.tocoo())\n",
    "keywords = extract_top_n_from_vector(feature_names, sorted_items, 50)\n",
    "keywords\n",
    "# 7 = pizza, italian\n",
    "# 4 = sweets (chocolate, cookies, cake, cream)\n",
    "# 5 = seafood?\n",
    "\n",
    "# user7 = asian\n",
    "# user0 = sweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKnt6Or6ipGh"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRfKxCdHgREf"
   },
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [0.090909, 0.1111111, 0.5],\n",
    "    [0.090909, 0, 0],\n",
    "    [0.11111111, 0, 0],\n",
    "    [0.11111111, 0, 0],\n",
    "    [0.11111111, 0.1818181818, 0],\n",
    "    [0.11111111, 0.090909, 0],\n",
    "    [0.25, 0, 0]\n",
    "])\n",
    "a_scaled = MinMaxScaler().fit_transform(a.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F5Qb-22kERf"
   },
   "outputs": [],
   "source": [
    "a_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeZdanXWjb46"
   },
   "outputs": [],
   "source": [
    "np.std(a_scaled,axis=1,ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymqkQdsajbzX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x7p6S8rjbtR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVRgsvpXjEEA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aD8KLO2US8LY"
   },
   "outputs": [],
   "source": [
    "### Move to Vectorizer_Builder (collapser version 2)\n",
    "\n",
    "#temp_reviews_df.sort_values([\"business_label\"],inplace=True)\n",
    "\n",
    "temp_labels = list(pd.unique(temp_reviews_df[label_column_name]))\n",
    "\n",
    "review_text_column = temp_reviews_df[\"review_text\"]\n",
    "review_text_column = clean_text_column(review_text_column)\n",
    "\n",
    "learned_tv = Pipeline([('count', CountVectorizer(vocabulary=learned_mapping)),\n",
    "                       ('tfidf', TfidfTransformer())])\n",
    "learned_tv.fit(review_text_column)\n",
    "tv_output = learned_tv.transform(review_text_column)\n",
    "\n",
    "tv_output_collapsed = []\n",
    "for temp_label in temp_labels:\n",
    "    temp_label_tv_output_collapsed = tv_output[np.where(temp_reviews_df[label_column_name]==temp_label)[0]].mean(axis=0)\n",
    "    tv_output_collapsed.append(np.squeeze(temp_label_tv_output_collapsed.A,0))\n",
    "tv_output_collapsed = np.array(tv_output_collapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo1ivBVlJ1aG"
   },
   "outputs": [],
   "source": [
    "ind = np.argsort(tv_output_collapsed, axis=1)\n",
    "[recipe_tfidf.idx2vocab[i] for i in ind[:,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u706hcFJ89EP"
   },
   "outputs": [],
   "source": [
    "def get_label_data(labels, id_list, id2vec, id_column_name, reviews_df):\n",
    "    labels_set = set(labels)\n",
    "    labels_representative_words = []\n",
    "    labels_representative_embeddings = []\n",
    "    labels_vocab_frequency = []\n",
    "    for label in labels_set:\n",
    "        temp_idxs = np.where(np.array(labels)==label)[0]\n",
    "        temp_ids = [id_list[temp_idx] for temp_idx in temp_idxs]\n",
    "        temp_label_df = reviews_df[reviews_df[id_column_name].isin(temp_ids)]\n",
    "        review_text_column = temp_label_df[\"review_text\"]\n",
    "        review_text_column = clean_text_column(review_text_column)\n",
    "        learned_mapping = dict(zip(list(recipe_tfidf.keys()),list(range(len(recipe_tfidf)))))\n",
    "        # learned_cv = CountVectorizer(vocabulary=learned_mapping)\n",
    "        # cv_output = learned_cv.transform(review_text_column)\n",
    "        # cv_output = cv_output.tocsc()\n",
    "        # scipy.sparse.csc_matrix.sum(cv_output, axis=0)\n",
    "\n",
    "        learned_tv = TfidfVectorizer(vocabulary=learned_mapping)\n",
    "        tv_output = learned_tv.transform(review_text_column)\n",
    "        tv_output.\n",
    "\n",
    "        review_text_column\n",
    "        for temp_idx in temp_idxs:\n",
    "            temp_id = id_list[temp_idx]\n",
    "            #for key in id2vec[temp_id]: embeddings_group += id2vec[(id_list[temp_idx])][key];\n",
    "            ##\n",
    "            temp_id_df = reviews_df[reviews_df[id_column_name]==temp_id]\n",
    "            review_text_column = temp_id_df[\"review_text\"]\n",
    "            review_text_column = clean_text_column(review_text_column)\n",
    "            review_text_column = review_text_column.apply(\n",
    "                lambda x: \" \".join([\n",
    "                    word for word in x.split(\" \") \n",
    "                    if word in recipe_tfidf.vocab2idx\n",
    "                    ])+\" \")\n",
    "            \n",
    "            combined_review_texts_of_id = review_text_column.values.sum() # get all re\n",
    "            group_vocab += (combined_review_texts_of_id+\" \")\n",
    "            ##\n",
    "        group_vocab_list = group_vocab.split(\" \")\n",
    "        group_vocab_list = [word for word in group_vocab_list if word != \"\"]\n",
    "        group_vocab_frequency = pd.Series(group_vocab_list).value_counts()\n",
    "        labels_vocab_frequency.append(group_vocab_frequency)\n",
    "\n",
    "\n",
    "        #embeddings_group = np.array(embeddings_group)\n",
    "        #labels_representative_embedding = embeddings_group.mean(axis=0)\n",
    "        #labels_representative_embeddings.append(labels_representative_embedding)\n",
    "\n",
    "        #labels_representative_word = recipe_word2vec.most_similar(positive=[labels_representative_embedding], topn=1)\n",
    "        #labels_representative_words.append(labels_representative_word[0][0])\n",
    "\n",
    "\n",
    "    return labels_set, labels_representative_words, labels_representative_embeddings, labels_vocab_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uvuYo6fElQa"
   },
   "outputs": [],
   "source": [
    "labels_set, labels_representative_words, labels_representative_embeddings = get_label_data(user_labels, user_id_list, user_business2reviewTextVec)\n",
    "labels_representative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EI3a6s0dPUsO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYph2MTjPUp_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exOmNI_3PUnY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaSuc25gPUkn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEGOckhAPUig"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFJxaRRxPUgK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMZ9l04sPUdz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKbOT2KLPUbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og4F_ZvhPUYy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYm3OYcUPUWp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73DjPzahPUUZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwtcyQiuPURx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGyyyFYiElE5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ8Hm4qRC5Lq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwGcHAUXbPrt"
   },
   "outputs": [],
   "source": [
    "temp_business_cluster_mapper = pd.DataFrame()\n",
    "temp_business_cluster_mapper[\"business_id\"] = business_id_list\n",
    "temp_business_cluster_mapper[\"business_id_regrouped\"] = business_labels\n",
    "\n",
    "temp_user_cluster_mapper = pd.DataFrame()\n",
    "temp_user_cluster_mapper[\"user_id\"] = user_id_list\n",
    "temp_user_cluster_mapper[\"user_id_regrouped\"] = user_labels\n",
    "\n",
    "temp_reviews_df = reviews_df.copy()\n",
    "temp_reviews_df = temp_reviews_df.merge(temp_business_cluster_mapper, on=[\"business_id\"])\n",
    "temp_reviews_df = temp_reviews_df.merge(temp_user_cluster_mapper,  on=[\"user_id\"])\n",
    "\n",
    "temp_reviews_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPWSEkgZbPpV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtniCLxMbPnN"
   },
   "outputs": [],
   "source": [
    "temp_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXVQuPk5bPky"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VU8VuZbbPia"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsS8ZmRCUrtb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAQAaXKPUrq7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdK7sXibUrZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5zou3VvXjcz"
   },
   "outputs": [],
   "source": [
    "# end goal (just for scatterplotting)\n",
    "# create a 2D scatterplot from (PCA, TSNE)\n",
    "# with grouped adjacent clusters (size>20, n_cluster<50) with representative word being adjacent\n",
    "# these clusters need not be made after the creation of the 2D scatterplotting,\n",
    "# for business and user (business easier, but user more useful)\n",
    "#\n",
    "# but how make words spacially logical? we have the pita-naan july4th word2vec model...\n",
    "# given (for user or business) \n",
    "# # a bank of review sentences, clean, split, remove stop words, keep only food-related words (how do that? with just ingredients, or names too (introduces the risk of emotional words))\n",
    "# given (for user or business) a bank of review sentences after data-cleaning, get a representative vector-point \n",
    "# # (which is then put through PCA/TSNE for a 2D scatterplot)\n",
    "# cluster group these user businesses or vector points\n",
    "\n",
    "\n",
    "\n",
    "# why do embeddings when words are faster \n",
    "# to embedding\n",
    "# micro \n",
    "# # DBSCAN\n",
    "# # Eventually, 2D\n",
    "# macro \n",
    "# # 2D scatterplot (TSNE)\n",
    "# # only here can tf-idf be applied"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
