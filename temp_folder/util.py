# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lnieii2o5KKsixekF6KAOJin3Aubldsp
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use("ggplot")

import os
from bs4 import BeautifulSoup  
import requests
import zipfile
import io
import nltk

#import geopandas as gpd
#import shapely
import gensim
import re
import tqdm
import tensorflow as tf
import keras
import string

def temp_interaction_metrics(temp_df, column_name_a, column_name_b):
    print("{0}s by {1}".format(column_name_b, column_name_a))
    b_by_a = temp_df.groupby([column_name_a],as_index=False).agg({column_name_b: set})
    print("{0} amount :".format(column_name_a), b_by_a.shape[0])
    b_by_a_len = b_by_a[column_name_b].apply(len)
    print("max {0} amount of a {1} :".format(column_name_b, column_name_a), b_by_a_len.max())
    print("median {0} amount of a {1} :".format(column_name_b, column_name_a), b_by_a_len.median())
    print("mean {0} amount of a {1} :".format(column_name_b, column_name_a), b_by_a_len.mean())
    fig, axs = plt.subplots()
    axs.hist(b_by_a_len)
    return b_by_a
#user_ids_by_business_ids = temp_interaction_metrics(reviews_df, "business_id", "user_id")

def temp_z_score_accom(x):
    temp_mean = np.mean(x)
    temp_std = np.std(x, ddof=0)
    if temp_std == 0:
        result = 0
    else:
        result = (x - temp_mean)/(temp_std)
    return result
def get_relative_groupwise_numbers(temp_df, group_column_name, numbers_column_name):
    result = temp_df.groupby([group_column_name])[numbers_column_name].transform(temp_z_score_accom)
    return result
def get_subjective_ratings(temp_df, user_id_column_name, rating_column_name):
    #rating_column = temp_df[rating_column_name]
    temp_means = temp_df.groupby([user_id_column_name])[rating_column_name].transform(np.mean)
    temp_stdevs = temp_df.groupby([user_id_column_name])[rating_column_name].transform(lambda x: np.std(x, ddof=0))
    #temp_stdev_numerator_presum = (rating_column - temp_means)**2
    #temp_stdev_numerator_sum = temp_df_.groupby([user_id_column_name])["temp_stdev_numerator_presum"].transform(sum)
    #temp_stdev_denominator = temp_df.groupby([user_id_column_name])[rating_column_name].transform(len)
    #temp_variance = 
    temp_z_score = temp_means / temp_stdevs
    temp_z_score_zerodiv = np.nan_to_num(temp_z_score, nan=0, posinf=0, neginf=0)
    result = temp_z_score_zerodiv
    return result
def get_normalized_ratings(temp_df, user_id_column_name, rating_column_name):
    rating_column = temp_df[rating_column_name]
    temp_means = temp_df.groupby([user_id_column_name])[rating_column_name].transform(np.mean)
    #temp_stdevs = temp_df.groupby([user_id_column_name])[rating_column_name].transform(lambda x: np.std(x, ddof=0))
    temp_stdev_numerator_presum = (rating_column - temp_means)**2
    temp_df_ = pd.DataFrame()
    temp_df_[user_id_column_name] = temp_df[user_id_column_name]
    temp_df_["temp_stdev_numerator_presum"] = temp_stdev_numerator_presum
    temp_stdev_numerator_sum = temp_df_.groupby([user_id_column_name])["temp_stdev_numerator_presum"].transform(sum)
    temp_stdev_denominator = temp_df.groupby([user_id_column_name])[rating_column_name].transform(len)
    temp_variance = temp_stdev_numerator_sum/temp_stdev_denominator
    temp_stdevs = np.sqrt(temp_variance)
    temp_z_score = temp_means / temp_stdevs
    temp_z_score_zerodiv = np.nan_to_num(temp_z_score, nan=0, posinf=0, neginf=0)
    result = temp_z_score_zerodiv
    return result

def word2vec_arithmetic(word2vec_model, group1, group1_version_word, group2):
    #group1 = "india" # man
    #group1_version_word = "naan bread" # king
    #group2 = "arabic" # woman
    title = ("{0} - {1} + {2} = ".format(group1_version_word, group1, group2))
    result = word2vec_model.wv.most_similar(positive=[group2, group1_version_word], negative=[group1])
    pd.Series(index=[x[0] for x in result], data=[x[1] for x in result])[::-1].plot(kind="barh", title=title)

def dataclean_strlistnum_to_listfloat(
    column: pd.Series
                                   ) -> pd.Series:
    """
    If the column has values that are strings of list of numbers, rather than list of numbers,
    this turns it into a list of floats. "[1, 2.0, 3.2]" becames [1.0, 2.0, 3.2]. 
    Args:
        column (pd.Series): column is pd.Series where each value is a string of a list of numbers
    Returns:
        pd.Series where each value is a list of floats
    """    
    result = (
    column
    ).str.strip("]["
    ).str.replace(",",""
    ).str.split(" "
    ).apply(lambda l: [float(x) for x in l])
    return result

def dataclean_explodes_list_to_cols(
    column: pd.Series
) -> pd.DataFrame:
    """
    If the column has lists as values, this outputs a dataframe that explodes the columns.
    EACH LIST IN THE COLUMN NEEDS TO HAVE THE SAME SIZE
    Args:
        column (pd.Series): column is pd.Series where each value is a list of the same size throughout
    Returns:
        pd.DataFrame
    """
    column_name = column.name
    amount = len(column.values[0])
    result = pd.DataFrame(np.array(column.tolist()),
                          columns=[str(column_name)+"_"+str(i) for i in range(amount)]
                         )
    return result

def dataclean_strliststr_to_liststr(
    column: pd.Series
                                   ) -> pd.Series:
    """
    If the column has values that are strings of list of strings, rather than list of strings,
    this turns it into a list of strings. "['a','b','c']" becames ['a','b','c']. 
    Args:
        column (pd.Series): column is pd.Series where each value is a string of a list of strings
    Returns:
        pd.Series where each value is a list of strings
    """
    result = (
      column
      ).str.strip("]["
      ).str.replace("""'""",""
      ).str.split(", "
    )
    return result

def dataclean_everygram_str_to_liststr(
    column: pd.Series,
    max_len = 1,
                                      ) -> pd.Series:
    """
    Applies everygrams onto the column of the raw_recipes csv. Uses whitespace to split.
    Example:
    if max_len = 2,then "grilled cheese sandwich" -> ["grilled", "grilled cheese", "cheese", "cheese sandwich", "sandwich"]
    Args:
        column (pd.Series): column is pd.Series where each value is raw text. 
    Returns:
        pd.Series where each value is a list of strings
    """    
    assert max_len >= 1
    column = (column).apply(lambda x: re.sub(" +", " ",str(x))).str.split(" ")
    result = column.apply(lambda s: [" ".join(list(t)) for t in nltk.everygrams(s, max_len=2)])
    return result

def dataclean_join_liststr_columns(
    list_of_columns: list
) -> pd.Series:
    """
    list_of_columns is List<pd.Series<List<String>>>
    This Horizontally concatenates a list of columns where the values are lists.
    Args:
        list_of_columns (list): list of columns
    Returns:
        pd.Series - a column where each value is a giant list, which is the row-wise concatenations of the list columns.
    """
    result = list_of_columns[0]
    for column in list_of_columns[1:]:
        result += (column)
    return result
def dataclean_joined_liststr_columns_to_str(
    liststr_column: pd.Series
) -> pd.Series:
    """
    liststr_column is pd.Series<List<String>>. This joins the strings
    Args:
        liststr_column (pd.Series): column of list of strings
    Returns:
        pd.Series - a column where each value is a string, which is the row-wise concatenations of the liststr columns.
    """
    result = (liststr_column).apply(lambda x: " ".join(x))
    return result
def dataclean_word2vec(
    column: pd.Series
                      ):
    """
    creates the word2vec model
    Args:
        column (pd.Series): column is pd.Series where each value is a list of strings. Word2Vec requires column to be this format
    Returns:
        gensim.models.Word2Vec
    """
    word2vec_model = gensim.models.Word2Vec(
            column,
            size=100,
            window=3,
            min_count=1,
            workers=10
    )
    return word2vec_model
def dataclean_doc2vec(
    column: pd.Series
                      ):
    """
    creates the Doc2vec model
    Args:
        column (pd.Series): column is pd.Series where each value is a list of strings. doc2Vec requires column to be this format
    Returns:
        gensim.models.doc2Vec
    """
    documents = [gensim.models.doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(list(liststr_columns.values))]
    doc2vec_model = gensim.models.Doc2Vec(
            documents,
            size=100,
            window=3,
            #min_count=1,
            #workers=10
    )
    return doc2vec_model

def clean_text_column(text_column):
    non_alphanumeric = string.punctuation # constant
    text_column = text_column.str.lower()
    text_column = text_column.str.translate(str.maketrans(non_alphanumeric, " "*len(non_alphanumeric)))
    return text_column